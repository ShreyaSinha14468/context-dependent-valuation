{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0YZag6Hg2y29sV3TR0MHP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShreyaSinha14468/context-dependent-valuation/blob/main/Context_dependent_valuation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_efP1r7FUIi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import statsmodels.api as sm\n",
        "import ast\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Connection to Drive"
      ],
      "metadata": {
        "id": "mu4m0WsO1-uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtJo7TsBTEAE",
        "outputId": "b91c4765-654d-4608-f6c4-e66da211a3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Connection to Git"
      ],
      "metadata": {
        "id": "yl2QWYgd15gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git\n",
        "!git config --global user.name \"ShreyaSinha14468\"\n",
        "!git config --global user.email \"ss14468@nyu.edu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJNIUmFJTkr9",
        "outputId": "180459a4-d507-4af8-c281-55720b459b54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.10).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ShreyaSinha14468/context-dependent-valuation.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCaGaOT-TugE",
        "outputId": "e33a452b-daa9-4b5e-e561-896f8dee6afd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'context-dependent-valuation' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# df.to_csv('your-repo/example.csv', index=False)\n",
        "os.chdir('context-dependent-valuation')\n",
        "!git status\n",
        "# !git add example.csv\n",
        "# !git commit -m \"Add example dataset\"\n",
        "# !git push"
      ],
      "metadata": {
        "id": "RH7I-wOHTxax",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9a2da1-619b-4e4c-d6c4-80da8a656dc2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWW3TugLSx8q",
        "outputId": "51cb43f8-27a2-4533-f067-429dbdc5f707"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/context-dependent-valuation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Generation"
      ],
      "metadata": {
        "id": "S63IM9_mF0Eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NoisySelectionDataset(Dataset):\n",
        "    def __init__(self, num_samples, covariance_matrix, num_values=3, mean_cov=[0,0,0], noise_sd=1/3, noise_mean = 0):\n",
        "        self.num_samples = num_samples\n",
        "        self.num_values = num_values\n",
        "        self.cov_matrix = covariance_matrix\n",
        "        self.mean_cov = mean_cov\n",
        "        self.noise_sd = noise_sd\n",
        "        self.noise_mean= noise_mean\n",
        "        self.dataset = self.generate_dataset()\n",
        "\n",
        "    def generate_dataset(self):\n",
        "        samples = []\n",
        "        for _ in range(self.num_samples):\n",
        "            true_values = np.random.multivariate_normal(self.mean_cov, self.cov_matrix)\n",
        "            noisy_values = true_values + np.random.normal(self.noise_mean, self.noise_sd, self.num_values)\n",
        "            best_index = np.argmax(true_values)\n",
        "            samples.append({'input': noisy_values, 'output': best_index, 'true_values': true_values})\n",
        "        return samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "        return {\n",
        "            'input': torch.tensor(sample['input'], dtype=torch.float32),\n",
        "            'output': torch.tensor(sample['output'], dtype=torch.long),\n",
        "            'true_values': torch.tensor(sample['true_values'], dtype=torch.float32)\n",
        "        }"
      ],
      "metadata": {
        "id": "lV7ytpJXFwXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(predictions, targets):\n",
        "    _, predicted = torch.max(predictions, 1)\n",
        "    correct = (predicted == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    accuracy = correct / total\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "So5uCGVdFwb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loss Plot\n",
        "def plot_train_loss(train_losses):\n",
        "    plt.plot(range(len(train_losses)), train_losses, label='Training Loss')\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "nCtLycSGP-aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation Accuracy Plot\n",
        "def plot_validation_accuracy(val_accuracies):\n",
        "    plt.plot(range(len(val_accuracies)), val_accuracies, label='Validation Accuracy')\n",
        "    plt.title(\"Validation Accuracy\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jtEp5GtaQSnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Loss Function:"
      ],
      "metadata": {
        "id": "B9ubRsJyMS0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss_reward_maximization(predictions, inputs, targets):\n",
        "    probs = torch.nn.functional.softmax(predictions, dim=1)\n",
        "    rewards = torch.sum(probs * inputs, dim=1)\n",
        "    # actual_rewards = torch.gather(inputs, 1, targets.view(-1, 1))\n",
        "    # diff = abs(actual_rewards - rewards)\n",
        "    # return torch.mean(diff)\n",
        "    return -torch.mean(rewards)"
      ],
      "metadata": {
        "id": "R9EUW7UuFwgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network Model:"
      ],
      "metadata": {
        "id": "5T133uxqMbG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_values = 3"
      ],
      "metadata": {
        "id": "k45Ej-y4MmKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Single Layer; 3 neuron units:"
      ],
      "metadata": {
        "id": "ZjD8VhHmM4Uf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Simple3NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Simple3NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_values, 3)\n",
        "        self.fc2 = nn.Linear(3, num_values)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "zC0LbFgBFwiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Specifications\n",
        "batch_size = 32\n",
        "model = Simple3NN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "epochs = 50\n",
        "train_losses = []\n",
        "val_losses = []"
      ],
      "metadata": {
        "id": "sdOZ7-LDNJkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Specifications\n",
        "num_train_samples = 10000\n",
        "num_val_samples = 1000\n",
        "num_values = 3\n",
        "covariance_matrix = np.array([[1, 0.1, 0.1], [0.1, 1, 0.1], [0.1, 0.1, 1]])\n",
        "mean_cov=[0,0,0]\n",
        "noise_sd=1/3\n",
        "noise_mean = 0\n",
        "\n",
        "dataset = NoisySelectionDataset(\n",
        "    num_samples=num_train_samples,\n",
        "    num_values=num_values,\n",
        "    covariance_matrix=covariance_matrix,\n",
        "    mean_cov=mean_cov,\n",
        "    noise_sd=noise_sd,\n",
        "    noise_mean=noise_mean\n",
        "    )\n",
        "\n",
        "# Sample Covariance Test\n",
        "inputs = np.vstack([sample['true_values'] for sample in dataset])\n",
        "covariance_matrix = np.cov(inputs, rowvar=False)\n",
        "print(\"Covariance Matrix:\\n\")\n",
        "print(covariance_matrix)"
      ],
      "metadata": {
        "id": "TWxPPjiqNYJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "\n",
        "    train_dataset = NoisySelectionDataset(\n",
        "        num_samples=num_train_samples,\n",
        "        num_values=num_values,\n",
        "        covariance_matrix=covariance_matrix,\n",
        "        mean_cov=mean_cov,\n",
        "        noise_sd=noise_sd,\n",
        "        noise_mean=noise_mean\n",
        "        )\n",
        "\n",
        "    dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    total_train_samples = 0\n",
        "    correct_train_samples = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        inputs = batch['input']\n",
        "        outputs = batch['output']\n",
        "\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        loss = criterion(predictions, outputs)\n",
        "        # loss = custom_loss_reward_maximization(predictions, inputs, outputs)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_train_samples += outputs.size(0)\n",
        "        correct_train_samples += (torch.argmax(predictions, dim=1) == outputs).sum().item()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "    train_accuracy = correct_train_samples / total_train_samples\n",
        "    print(f'Epoch: {epoch + 1}/{epochs}, Train Loss: {loss.item()}, Train Accuracy: {train_accuracy}')"
      ],
      "metadata": {
        "id": "UNMRDj9FNPsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_loss(train_losses)"
      ],
      "metadata": {
        "id": "Bj_-5DsiOvpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "val_dataset = NoisySelectionDataset(\n",
        "        num_samples=num_val_samples,\n",
        "        num_values=num_values,\n",
        "        covariance_matrix=covariance_matrix,\n",
        "        mean_cov=mean_cov,\n",
        "        noise_sd=noise_sd,\n",
        "        noise_mean=noise_mean\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
        "with torch.no_grad():\n",
        "        val_losses = []\n",
        "        val_accuracies = []\n",
        "        for val_batch in validation_dataloader:\n",
        "            val_inputs = val_batch['input']\n",
        "            val_outputs = val_batch['output']\n",
        "            val_predictions = model(val_inputs)\n",
        "            # val_loss = custom_loss_reward_maximization(val_predictions, val_inputs, val_outputs)\n",
        "            val_loss = criterion(val_predictions, val_outputs)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "            val_accuracy = calculate_accuracy(val_predictions, val_outputs)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "\n",
        "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
        "        avg_val_accuracy = sum(val_accuracies) / len(val_accuracies)\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs}, Validation Loss: {avg_val_loss}, Validation Accuracy: {avg_val_accuracy}')"
      ],
      "metadata": {
        "id": "eIqfCLKZOPlE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_validation_accuracy(val_accuracies)"
      ],
      "metadata": {
        "id": "lXITRhtgOVtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### Regression and RVI Analysis:"
      ],
      "metadata": {
        "id": "RojFeYpQQsvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coefficients_df = pd.DataFrame(columns=['# Neurons', 'V1', 'V2', 'V3', 'Const', 'RVI'])"
      ],
      "metadata": {
        "id": "zhb51d_oOstn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "train_dataset = NoisySelectionDataset(num_samples=num_train_samples)\n",
        "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "with torch.no_grad():\n",
        "    input_ = torch.cat([batch['input'] for batch in dataloader])\n",
        "    hidden_activations1 = model.fc1(input_).numpy()\n",
        "    output_activations = model.fc2(model.tanh(model.fc1(input_))).numpy()"
      ],
      "metadata": {
        "id": "IA2CtihMQxej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_neurons = len(hidden_activations1[0])\n",
        "for i in range(num_neurons):\n",
        "    X = sm.add_constant(input_.numpy())\n",
        "    reg_model = sm.OLS(hidden_activations1[:, i], X)\n",
        "    reg_model_results = reg_model.fit()\n",
        "\n",
        "    params = reg_model_results.params\n",
        "\n",
        "    coeff_list = np.array([params[1], params[2], params[3]])\n",
        "    max_coeff_idx = np.argmax(coeff_list)\n",
        "    mask = np.ones(coeff_list.size, dtype=bool)\n",
        "    mask[max_coeff_idx] = False\n",
        "    sum_others = np.sum(coeff_list[mask])\n",
        "    RVI = -sum_others/coeff_list[max_coeff_idx]\n",
        "\n",
        "    list_row = [num_neurons, params[1], params[2], params[3], params[0], RVI]\n",
        "    coefficients_df.loc[len(coefficients_df)] = list_row\n",
        "\n",
        "    coefficients_df.to_csv(f'Low_Cover_One_Hidden_Layer_Regression_Results.csv', index=False)\n"
      ],
      "metadata": {
        "id": "4p1KhUa4Qxhu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "3adcbca1-5a9c-4e84-a9f5-06719393d160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hidden_activations1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-6728414a5475>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnum_neurons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_activations1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_neurons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mreg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_activations1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mreg_model_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hidden_activations1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Single Layer; 6 neuron units:"
      ],
      "metadata": {
        "id": "JGBbrg7SNAqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Simple6NN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Simple6NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(num_values, 6)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(6, num_values)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.tanh(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EfaxXMFTFwk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RC_qq2gJFwm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lhkhYwmoFwrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f70nKkPdFwuM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PebNLwvLFwwT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}